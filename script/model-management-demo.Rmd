---
title: "Model Management"
description: > 
  Example model submission workflow with <font class=mrgc>bbr</font>.
#image: bbr-strip.png
order: 500
categories: 
- bbr
- model management
- nonmem
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  cache = TRUE, 
  autodep = TRUE, 
  comment = '.', 
  message = FALSE,
  warning = FALSE,
  out.width = 500, 
  out.height = 750
)
```

# Introduction

Managing the model development process in a way that leads to a traceable and reproducible analysis can be a significant challenge. At MetrumRG, we use `bbr` to streamline this process. `bbr` is an R package developed by MetrumRG that serves three primary purposes:

* Submit NONMEM models, particularly for execution in parallel and/or on a high-performance compute (HPC) cluster (e.g. {{< var pages.about_metworx >}}).
* Parse NONMEM outputs into R objects to facilitate model evaluation and diagnostics in R.
* Annotate the model development process for easier and more reliable traceability and reproducibility.

This page will demonstrate using `bbr` to do the following:

* Create and submit a model
* Simple model evaluation and diagnostics
* Iterative model development
* Annotation of models with tags, notes, etc.


# Tools used

## MetrumRG packages
{{< var used.bbr >}}

## CRAN packages
{{< var used.dplyr >}}

```{r}
library(bbr)
library(dplyr)
library(here)

MODEL_DIR <- here("model", "pk")
FIGURE_DIR <- here("deliv", "figure")
```

```{r, eval = TRUE, include = FALSE}
#' This section is not visible in the article version of this script.
#' However, RUN THIS CHUNK if you would like to step through this code interactively.
#'
#' Most importantly, it creates a new model directory with your name
#' so that you can modify and submit these models without touching to originals.

# create scratch model dir for user
orig_model_dir <- here("model", "pk")
user_name <- Sys.getenv("USER")
if (!nzchar(user_name)) user_name <- "user"
MODEL_DIR <- paste(orig_model_dir, user_name, sep = "-")
if (fs::dir_exists(MODEL_DIR)) fs::dir_delete(MODEL_DIR)
fs::dir_create(MODEL_DIR)

# copy through starting files
fs::file_copy(file.path(orig_model_dir, "bbi.yaml"), file.path(MODEL_DIR, "bbi.yaml"))
fs::file_copy(file.path(orig_model_dir, "100.ctl"), file.path(MODEL_DIR, "100.ctl"))

# If on Metworx, ping the grid to bring up a compute node.
# If not on Metworx, set mode to "local" for easier interactive model submission.
if (nzchar(Sys.getenv("METWORX_VERSION"))) {
  system("echo 'sleep 10' | qsub")
} else {
  options("bbr.bbi_exe_mode" = "local")  
}

# Check that bbi is installed
if (!nzchar(bbi_version())) use_bbi()

#' This function is used to "update" your control streams by modifying them
#' to match the relevant control stream in the original model directory.
update_demo_ctl <- function(.mod, orig_model_dir = here::here("model", "pk")){
  
  new_ctl_path <- get_model_path(.mod)
  
  mod_to_copy <- bbr::read_model(file.path(orig_model_dir, bbr::get_model_id(.mod)))
  orig_ctl_path <- get_model_path(mod_to_copy)
  
  # overwrite ctl
  if(fs::file_exists(new_ctl_path)) fs::file_delete(new_ctl_path)
  fs::file_copy(orig_ctl_path, dirname(new_ctl_path))
  
  return(invisible(.mod))
}
```

# Outline

The purpose of this page is to show a wide range of things that a scientist may want to do throughout the modeling process. However, a file like this is generally _not_ kept in the repo of an actual project. Typically, scientists use a file like {{< var expo_repo.model_mang >}} as a scratch pad for defining, submitting, tagging, etc. their models, but it is _not_ necessary to persist the entire lineage, as we have done here. Again, we show this only to demonstrate the different steps that might be taken along the way. There is also a {{< var pages.model_summary >}} page, which shows how you can leverage the model annotation demonstrated here to summarize your modeling work.

We have a included a runnable version of this page as {{< var expo_repo.model_mang_demo >}} in our Github repository. However, if you are new to `bbr`, be sure to read through the ["Getting Started" vignette](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html) before getting too deep into that code. There is some setup and configuration required (e.g. making sure `bbr` can find your NONMEM installation) that is described in that vignette.

And finally, please note that **`bbr` does not edit the model structure in your control stream.** Below you will see comments asking you to edit your control stream manually. However, this Expo contains a sequence of models that we use to demonstrate how your control streams might evolve throughout a simple modeling process. 

# Model creation and submission

You begin this analysis with a fairly simple model. Before doing anything in `bbr`, you will need to have a control stream. You have named your first control stream `100.ctl`:

```{r comment = "", warning = FALSE}
cat(readLines(file.path(MODEL_DIR, "100.ctl")), sep = "\n")
```

## Create a model object

Once you have your control stream, you are ready to create a model object. You can read more about this object, and the call that creates it, in the [_Create model object_ section of the "Getting Started" vignette](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#create-model-object). Here you can use `bbr::new_model()` to create the model.

```{r, eval = TRUE}
# create the first model
mod100 <- new_model(file.path(MODEL_DIR, 100))
```

At minimum, know that the created object (in this case `mod100`) is an S3 object which you will pass to all of the other `bbr` functions that submit, summarize, or manipulate models. The first argument to `new_model()` is the path to your model control stream _without file extension_. The call above assumes you have a control stream named `100.ctl` (or `100.mod`) in the directory you have assigned to `MODEL_DIR`. 

### The model YAML file

Before you have called `new_model()`, you will only have your control stream and the `bbi.yaml` configuration file (discussed below) in your modeling directory.


The `new_model()` function will create a `100.yaml` file in the same folder, which will automatically persist any tags, notes, and other model metadata. 

It's useful to know that these files exist, but it is important to note that you should generally _not_ be interacting with these files directly (e.g. in the Rstudio files tab). `bbr` provides a variety of functions (i.e. `add_tags()` and `add_notes()`, shown below) that operate on a model object and automatically persist any changes in this YAML file.

### The `bbi.yaml` file

Global settings used by `bbr` are stored in a file named `bbi.yaml`. We won't discuss this file in detail here, but you can read more about it in the [_`bbi.yaml` configuration file_ section of the "Getting Started" vignette](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#bbi-yaml-configuration-file).

## Submitting a model

Now that you have created your model object, you can submit it to be run with `bbr::submit_model()`.

```{r, eval = TRUE}
submit_model(mod100)
```

There are a number of customizations available for submitting models that we won't discuss here. Check the docs for [`?submit_model`](https://metrumresearchgroup.github.io/bbr/reference/submit_model.html) and [`?modify_bbi_args`](https://metrumresearchgroup.github.io/bbr/reference/modify_bbi_args.html) for more detailed information.

### Monitoring submitted models

Now that you have submitted your model, you have to wait until it finishes running before you can move on. Feel free to go get some coffee. If you've submitted your model to the SGE grid (the default) you can use `system("qstat")` to check on your jobs. `bbr` also provides some simple helpers for peeking at (by default, the first 3 and last 5 lines) of some relevant files, for the purpose of checking on runs in progress.

```{r}
tail_lst(mod100)
```

Oh good! It looks like it's done.

# Summarize model outputs

Once your model is finished, you may want to look at some diagnostics and outputs from the model. The `bbr::model_summary()` function returns an object that contains information parsed from a number of files in the output folder.

```{r}
# load the summary object
sum100 <- model_summary(mod100) 
names(sum100)

# this is basically everything from the .lst file, plus some others...
sum100$run_details$output_files_used
```

## Simple parameter table

You can also explore this object directly (i.e. with `View(sum100)`, it's just a named list) but often it's more useful to use one of `bbr`'s helper functions, for example to quickly construct a basic parameter table:

```{r}
# helper function to extract parameter table
sum100 %>% param_estimates()
```

## Printing a high-level summary

If you print this object to the console, you will get nice high-level summary of the model performance. Doing this in an `.Rmd` chunk with the chunk option `results = 'asis'` will also print nicely in the rendered document:

```{r, results = 'asis'}
sum100
```

## Investigating heuristics

Notice that the summary printed above shows that several of the heuristic flags were triggered. These are not _errors_ per se, but things that should be checked because they could indicate an issue with the model.

The documentation in [`?bbr::model_summary()`](https://metrumresearchgroup.github.io/bbr/reference/model_summary.html#details) describes each of these heuristics. There are also helper functions like `bbr::nm_grd()` that will pull the gradient file into a tibble. And remember that things like the condition number can be extracted from the summary object itself with `sum100$condition_number`.

If you see the `PRDERR` flag in the heuristics section, that means there was a `PRDERR` file generated by NONMEM for this run. You can use `file.path(get_output_dir(sum100), "PRDERR")` to easily get the path to that file, and take a look at it with `file.edit()` or `readLines()` to see what the error was.

## Diagnostic plots

You can also render a suite of pre-defined diagnostic plots for your model, as described in {{< var pages.parameterized_reports >}}. In addition to saving a suite of diagnostic plots to a `diagnosticPlots_{run}.html` file in your model output folder. It will also save the individual plots to `.pdf` files in `deliv/figure/{run}/`.

For your first model, you can browse some of the plots. You notice, among other things, this `CWRES` plot:

```{r, eval = TRUE}
browseURL(file.path(FIGURE_DIR, "100/100-cwres-pred-time-tad.pdf"))
```

# Annotating your model

One of the more useful features of `bbr` is the ability to easily annotate your model. This helps you keep track of your modeling process as you go, and can be easily retrieved later for creating "run logs" that describe the entire analysis (shown later). 

A well-annotated model development process does several important things:

* Makes it easy to update collaborators on the state of the project.
* Supports reproducibility and traceability of your results.
* Keeps the project organized, in case you need to return to it at a later date.

The `bbr` model object contains three annotation fields. The `description` is a character scalar (i.e. a single string), while `notes` and `tags` can contain character vectors of any length. These can be used for any purpose, but you will see some recommended patterns demonstrated below.

## Using `description`

The `description` field defaults to `NULL` and is generally only filled in for particularly notable models. One benefit of this approach is that it lets you easily [filter your run logs]((https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#run-log)) to only important models by using `dplyr::filter(!is.null(description))`.

Descriptions should typically be something terse like "Base model" or "First covariate model". You will see the `add_description()` function used for this on `mod102` below.

## Using `notes`

Notes are generally used for free text observations about a particular model. Some modelers leverage notes as "official" annotations that might get pulled into a run log in a final report of some kind, while other modelers prefer to keep them informal and use them primarily for their own reference.

Based on the plot above, you might add a note like this to your model, and then move on to a two-compartment model (model 101, in the next section).

```{r, eval = TRUE}
mod100 <- mod100 %>% 
  add_notes("systematic bias, explore alternate compartmental structure")
```

## Using `tags`

In contract to `notes`, [`tags`](https://metrumresearchgroup.github.io/bbr/reference/modify_tags.html) can be used to document your modeling work in a more structured way. You can see some of the benefits of this tagging strategy by looking at the {{< var pages.model_summary >}} article in this Expo, or the [_Run log_ section of the "Getting Started" vignette](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#run-log).

While `bbr` accepts a character vector for all tag-related arguments and functions, we highly recommend defining a "glossary" of tags that you will be using. This can be modified and added to throughout the course of the project. The important point is that **tags will become useless if they are inconsistently used**; for example, if sometimes you use `"one-compartment"` and other times you use `"one cmpt"`. Using an external glossary prevents this. This Expo repository contains a `tags.yaml` file with some example tags, but again, please customize this for your own project.

```{r load tags}
TAGS <- yaml::read_yaml(here("script", "tags.yaml"))
str(TAGS)
```

### Auto-complete for tags

Reading tags into a named list, as shown above, has the added benefit of enabling you to use Rstudio's auto-complete feature by typing `TAGS$` to see what tags are available, and filtering them by typing part of the tag you are looking for. For instance, try typing `TAGS$eta` in the console to view the relevant tags that have been defined.

### Add tags to your model

You can add tags at any time, before or after submitting your model. Before moving on, you might add a few tags to your first model.

```{r}
mod100 <- mod100 %>%
  add_tags(c(
    TAGS$one_compartment_absorption,
    TAGS$eta_cl,
    TAGS$eta_ka,
    TAGS$eta_v,
    TAGS$proportional_ruv
  ))
```

You can see your new tags when you print the model object (`mod100`) to the console, or when you call `run_log()` (described later). You can also see that the tags have been persisted in the YAML file on disk.

```{r}
cat(readLines(file.path(MODEL_DIR, "100.yaml")), sep = "\n")
```

# Iterative model development

Now you are ready to move on to your next model. To create a new model based on a previous model, use the `bbr::copy_model_from()` function. This will do several things:

* Copy the control stream associated with `.parent_mod` and rename it according to `.new_model`.
* Create a new model object associated with this new control stream.
* Fill the `based_on` field in the new model object (see the ["Using based_on" vignette](https://metrumresearchgroup.github.io/bbr/reference/articles/using-based-on.html) for more on that).
* Optionally inherit any tags from the parent model.

## Using `copy_model_from()`

You can see [`?copy_model_from`](https://metrumresearchgroup.github.io/bbr/reference/copy_model_from.html) or the [_Iteration_ section of the "Getting Started" vignette](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#iteration) for more details.

```{r, eval = TRUE}
mod101 <- copy_model_from(
    .parent_mod = mod100,
    .new_model = 101,
    .inherit_tags = TRUE
  ) 
```

Though you are explicitly passing `.new_model = 101` here, you can omit this argument if you are naming your models numerically, as in this example. if `.new_model` is _not_ passed, `bbr` will default to incrementing the new model name to the next available integer in the same directory as `.parent_mod`.

### Edit your control stream

You now have to manually edit the new control stream with any modifications that you want to make to the model structure. `bbr` has several helper functions to make this easier.

```{r overwrite ctl 1, eval = TRUE, include = FALSE}
# In place of manually editing the control steam, run this function to update from example models contained in this repo
update_demo_ctl(mod101)
```

```{r, eval = TRUE}
# updates any of the defined suffixes in your new control stream
# from `{parent_mod}.{suffix}` to `{current_mod}.{suffix}`
# Defaults to updating the following:
# * .MSF
# * .EXT
# * .CHN
# * .tab
# * par.tab
mod101 <- update_model_id(mod101)

# shows the difference between control streams of a model and it's parent model
model_diff(mod101)
```

If you're using Rstudio, you can also easily open your control stream for editing with the following:

```{r, eval = TRUE}
# opens the control stream for editing in Rstudio
mod101 %>%
  get_model_path() %>%
  file.edit()
```

### Submit the model and update tags

Once you are satisfied with your new control stream, submit the model to be run. 

Notice that here you can also update the tags on the model object, by replacing some of tags from the original model with new ones that reflect the changes you made to the control stream. You can see the benefit of using this tagging strategy when we construct run logs and summary tables later, as well as in the {{< var pages.model_summary >}} article in this Expo.

```{r, eval = TRUE}
# submit the model to be run
submit_model(mod101)

# similar to stringr::str_replace => old, new
mod101 <- mod101 %>%
  replace_tag(TAGS$one_compartment_absorption, TAGS$two_compartment_absorption) %>%
  replace_tag(TAGS$eta_v, TAGS$eta_v2)

```

**You can modify tags at any time**, either before or after you have submitted a model to run. The changes are persisted on disk in the model's YAML file and will also be reflected in the model object in memory. This is true of modifying notes and any other model attribute as well.

A final note on syntax: you need to re-assign to the model object to store the modifications made to the tags. In other words, be sure to do `mod <- mod %>% replace_tag(...)` instead of just `mod %>% replace_tag(...)`. This is true of notes and any other modifications made to the model object as well.

### Add notes and move on

After looking at the outputs and diagnostics (as described in the previous section) you notice a correlation between `eta-V2` and weight, and decide to add a note before moving on to the next model.

```{r, eval = TRUE}
browseURL(file.path(FIGURE_DIR, "101/101-eta-all-cont-cov.pdf"))
```

```{r, eval = TRUE}
mod101 <- mod101 %>% 
  add_notes("eta-V2 shows correlation with weight, consider adding allometric weight")
```

## Adding covariates

Next you can create a new model based on 101 and add some covariates. Notice that you can use `add_tags()` instead of `replace_tag()` because these new tags do not replace any previous tag. See [`?modify_tags`](https://metrumresearchgroup.github.io/bbr/reference/modify_tags.html) for details on the all different functions for interacting with tags.

```{r, eval = TRUE}
# create new model by copying mod 101
mod102 <- copy_model_from(
    mod101,
    .new_model = 102,
    .inherit_tags = TRUE
  ) %>% 
  add_tags(c(
    TAGS$cov_cl_wt_fixed,
    TAGS$cov_v2_wt_fixed,
    TAGS$cov_q_wt_fixed,
    TAGS$cov_v3_wt_fixed
  ))
```

```{r overwrite ctl 2, eval = TRUE, include = FALSE}
# In place of manually editing the control steam, run this function to update from example models contained in this repo
update_demo_ctl(mod102)
```

```{r, eval = TRUE}
# look at the difference between mod102 and its parent (mod101)
model_diff(mod102)
```

Then you submit the model:

```{r, eval = TRUE}
submit_model(mod102)
```

And, after looking at some diagnostics (as shown in the Model 100 section), you add a note before moving on.

```{r, eval = TRUE}
mod102 <- mod102 %>% 
  add_notes("Allometric scaling with weight reduces eta-V2 correlation with weight. Will consider additional RUV structures")
```

# Comparing error structures

At this point you might decide that you want to test two different error structures. This is a potential "fork in the road" so both of these models use `mod102` as the `.parent_mod` argument to `copy_model_from()`.

```{r, eval = TRUE}
mod103 <- copy_model_from(
    mod102,
    .new_model = 103,
    .inherit_tags = TRUE
  ) %>%
  replace_tag(TAGS$proportional_ruv, TAGS$additive_ruv)

mod104 <- copy_model_from(
    mod102,
    .new_model = 104,
    .inherit_tags = TRUE
  ) %>%
  replace_tag(TAGS$proportional_ruv, TAGS$combined_ruv)

# edit the control streams manually...
```

```{r overwrite ctl 3 & 4, eval = TRUE, include = FALSE}
# In place of manually editing the control steam, run this function to update from example models contained in this repo
update_demo_ctl(mod103)
update_demo_ctl(mod104)
```

```{r, eval = TRUE}
# model 103 starts from 102 and changes to additive error
# (you also update some parameter estimates)
model_diff(mod103)
```

```{r, eval = TRUE}
# model 104 starts from 102 and changes to combined error
model_diff(mod104)
```

## submit models in batch

In addition to `submit_model()`, which takes a single model object, there is also `bbr::submit_models()` which takes a list of model objects. This is especially useful for doing things like running a bootstrap, or for re-running a batch of models, as shown at the bottom of this page. However, it can also be used to submit two models at once, as shown here.

```{r, eval = TRUE}
submit_models(list(mod103, mod104))
```

## Compare objective function with `summary_log()`

The `bbr::summary_log()` function essentially runs `bbr::model_summary()` (shown in the Model 100 section above) on _all_ models in the specified directory, and parses the outputs into a tibble. This makes it very useful for comparing multiple models at any point in the project. You can look at [`?summary_log`](https://metrumresearchgroup.github.io/bbr/reference/summary_log.html) to see details on what is parsed into this `bbi_summary_log_df` tibble. Please also check out the ["Creating a Model Summary Log" vignette](https://metrumresearchgroup.github.io/bbr/articles/using-summary-log.html) for even more about how to effectively use this powerful function. 

```{r}
# use .recurse = FALSE to exclude the models in ../model/pk/boot
summary_log(MODEL_DIR, .recurse = FALSE) %>% 
  filter(run %in% 102:104) %>%
  select(
    run,
    ofv,
    param_count,
    condition_number,
    any_heuristics
  )
```

From looking at these metrics, you can begin to make some decisions about the error structures you have tried, and to make notes about those decisions. 

```{r, eval = TRUE}
mod103 <- mod103 %>% 
  add_notes("Additive only RUV performs poorly and will not consider going forward.")

mod104 <- mod104 %>% 
  add_notes("Combined RUV performs only slightly better than proportional only, should look at parameter estimates for 104")
```

## Filter `param_estimates()` to only `SIGMA`

Next you can use the `bbr::param_estimates()` function (which takes the `bbi_nonmem_summary` object, returned from `model_summary()`) to look at the `SIGMA` parameters you have estimated on your `mod104`.

```{r}
mod104 %>%
  model_summary() %>%
  param_estimates() %>%
  filter(grepl("SIGMA", parameter_names))
```

Based on these parameter estimates, you might decide not to proceed with the combined RUV because the additive error parameter contains zero. You can make a note of this, and additionally add a "description" to `mod102` to denote that it will be used for your base model. 

```{r, eval = TRUE}
mod104 <- mod104 %>% 
    add_notes("CI for Additive error parameter contains zero and will not be used going forward")

mod102 <- mod102 %>% 
  add_notes("Proportional RUV performed best over additive (103) and combined (104)") %>% 
  add_description("Base Model")
```

# Comparing final models

As you near the end of your modeling process, you might decide to add some pre-specified covariates to your base model. 

```{r, eval = TRUE}
mod105 <- copy_model_from(
    mod102,
    .new_model = 105,
    .inherit_tags = TRUE
  ) 
```

```{r overwrite ctl 5, eval = TRUE, include = FALSE}
# In place of manually editing the control steam, run this function to update from example models contained in this repo
update_demo_ctl(mod105)
```

```{r, eval = TRUE}
# edit the control stream manually...
model_diff(mod105)
```

## Adding `bbi` Arguments and Parallelization

You can also attach `bbi` arguments to the model object. By doing this, any time this model is submitted, these attached arguments will be applied. See the section on [_Passing arguments to bbi_ in the "Getting Started" vignette](https://metrumresearchgroup.github.io/bbr/articles/getting-started.html#passing-arguments-to-bbi) for more details, and a list of valid arguments. This is also **how you run your model in parallel** (See [the "Running NONMEM in Parallel: bbr Tips and Tricks" vignette](https://metrumresearchgroup.github.io/bbr/articles/nonmem-parallel.html) for details).

```{r, eval = TRUE}
# add arguments to be passed through to bbi
mod105 <- mod105 %>% add_bbi_args(list(clean_lvl = 0))

# submit the model to be run
submit_model(mod105)

# now update tags and notes while you wait for the model to run
mod105 <- mod105 %>%
  add_tags(c(
    TAGS$cov_cl_egfr,
    TAGS$cov_cl_age
  )) %>%
  add_notes("Pre-specified covariate model")
```

## Making final adjustments

Here you decide to try one more model, because one of your collaborators was interested in adding Albumin to CL.

```{r, eval = TRUE}
mod106 <- copy_model_from(
    mod105,
    .new_model = 106,
    .inherit_tags = TRUE
  ) %>% 
  add_tags(TAGS$cov_cl_alb) %>%
  add_notes("Collaborator was interested in adding Albumin to CL")

```

You edit the control stream manually and submit the model.

```{r overwrite ctl 6, eval = TRUE, include = FALSE}
# In place of manually editing the control steam, run this function to update from example models contained in this repo
update_demo_ctl(mod106)
```

```{r, eval = TRUE}
# submit the model to be run
submit_model(mod106)
```

Next you must decide whether 106 is an improvement on 105, and whether you should keep it as the final model. 

### Model heuristics

First, you can use `bbr::summary_log()` to look at the objective function and check whether there are any heuristics of concern with either model. (You can see more about which heuristics are checked in [`?summary_log`](https://metrumresearchgroup.github.io/bbr/reference/summary_log.html))

```{r}
sum_df <- summary_log(MODEL_DIR, .recurse = FALSE) 

sum_df %>% 
  filter(run %in% 105:106) %>%
  select(run, ofv, condition_number, any_heuristics)
```

In this case, you see no heuristics have been triggered for either model.

### Comparing parameter estimates

Next you might compare the parameter estimates between the two models. You can use `bbr::param_estimates()` to generate a tibble for each model, and then join them together with some help from `dplyr`:

```{r}
params106 <- mod106 %>% model_summary() %>% param_estimates() %>% select(1:3)
params105 <- mod105 %>% model_summary() %>% param_estimates() %>% select(1:3)

full_join(
  params106,
  params105,
  by = "parameter_names",
  suffix = c("_106", "_105")
) %>%
  mutate(across(where(is.numeric), pmtables::sig))
```

### Review diagnostic plots

_TODO: Update this section based on the "Model Diagnostics" article once it's written... Link to {{< var pages.model_diagnostics >}} and {{< var pages.parameterized_reports >}}_ 

As mentioned above, a number of diagnostic plots can be easily created using the diagnostic templates and `model-diagnostics.R` script. In this example, you have generated the interactive html in `model/pk/{run}` and saved PDFs out to the `deliv/figure` folder. You can compare the `_eta_pairs.pdf` plots between the two models.

**Model 105**
```{r, eval = TRUE}
browseURL(file.path(FIGURE_DIR, "105/105-eta-pairs.pdf"))
```

**Model 106**
```{r, eval = TRUE}
browseURL(file.path(FIGURE_DIR, "106/106-eta-pairs.pdf"))
```

### Choose final model

After checking all of this, you are satisfied that 106 will be your final model. You add this in the `description` field so that it can easily be identified as a key model when looking at the run log later.

```{r, eval = TRUE}
mod106 <- mod106 %>%
  add_description("Final Model")
```

# Model summary `.Rmd`

Several times throughout this document we have mentioned the {{< var pages.model_summary >}} article and accompanying {{< var expo_repo.model_summary >}} file. The intent is for that file to serve as a high-level overview of the project. The top section calls `bbr::run_log()`, which will pull in all tags, notes, etc. (from the model YAML files on disk) as they are whenever {{< var expo_repo.model_summary >}} is run or knit.

```{r}
# create a run log and do some basic formatting
run_log(MODEL_DIR, .recurse = FALSE) %>%
  collapse_to_string(based_on, tags, notes) %>%
  select(run, based_on, description, tags, notes)
```


The second section of `model-summary.Rmd`, titled `Modeling notes`, should be filled in at your discretion. The intent is _not_ to create a rough draft of a full report, but instead to capture some notes and relevant plots or tables at _key decision points_ in the modeling process. Since there are only seven models in this Expo, you will see all of them represented in that section. However, in a real project you would **only add sections for important models** that were in some way pivotal or notable for the direction of the project.

# Epilogue: submit models to be re-run

There are cases when you may have to re-run some or all of your models, for example if your data has changed. 

You can use the `bbr::config_log()` function to see if any model files or data files have changed since the model was last run; an indication that the model may need to be re-run. `bbr` also provides some helpful functionality for keeping track of model lineage (for example, in order to know _which_ models you may need to re-run). You can see some examples of both of these in the ["Using the based_on field" vignette](https://metrumresearchgroup.github.io/bbr/articles/using-based-on.html).

To re-run a model, simply add `.bbi_args = list(overwrite = TRUE)` to the `submit_model()` call.

```{r, eval = TRUE}
submit_model(mod100, .bbi_args=list(overwrite=TRUE))
```

To re-run a _group of_ models, use `submit_models()` with `.bbi_args = list(overwrite = TRUE)`.

```{r, eval = TRUE}
mods <- purrr::map(
  file.path(MODEL_DIR, 100:106), 
  read_model
)
submit_models(mods, .bbi_args=list(overwrite=TRUE))
```

# Other resources
<hr />

The following script from the {{< var expo_repo.main >}} are discussed on this page. If you're interested running this code, visit the {{< var pages.about_the_repo >}} page first.

Model management "scratch pad" script: {{< var expo_repo.model_mang >}}
\
Runnable version of the examples on this page: {{< var expo_repo.model_mang_demo >}}

## More `bbr` resources

* {{< var vignette.bbr >}}
* {{< var pkg_resource.bbr_cheatsheet >}}
* {{< var repo.bbr >}}
